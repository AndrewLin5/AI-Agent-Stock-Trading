{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cc0660-4ae4-49ee-af4e-26e42268513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent AI: To make decision making for the database\n",
    "# Environment for the external system\n",
    "# State is the represetation for the information available: Stock closing prices, moving averages, and daily returns\n",
    "# Action Space for AI Agent's actions: Buy, Sell, and Hold\n",
    "# Reward Function to determine the Agent's preformance by assigning a numerical value for it's actions: total profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d437f6ad-62e3-424b-9a49-fc3a9f4ed871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.61-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (2.4.2)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.1.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 3.0/3.0 MB 58.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.11.1-cp39-abi3-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from yfinance) (4.25.3)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\megaw\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Downloading yfinance-0.2.61-py2.py3-none-any.whl (117 kB)\n",
      "Downloading curl_cffi-0.11.1-cp39-abi3-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 77.0 MB/s eta 0:00:00\n",
      "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.18.1-py3-none-any.whl size=139097 sha256=5878db1d873098b6724ed978106428babbb66dd658e14a1770a3636459835968\n",
      "  Stored in directory: c:\\users\\megaw\\appdata\\local\\pip\\cache\\wheels\\1a\\57\\6a\\bb71346381d0d911cd4ce3026f1fa720da76707e4f01cf27dd\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, websockets, curl_cffi, yfinance\n",
      "Successfully installed curl_cffi-0.11.1 multitasking-0.0.11 peewee-3.18.1 websockets-15.0.1 yfinance-0.2.61\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3cb1107c-a41f-4005-ad17-b11f393c025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# define a specific stock symbol (Apple in this case) and time period\n",
    "symbol = \"TSM\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-06-01\"\n",
    "\n",
    "# download historical data\n",
    "data = yf.download(symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "394fe4d7-b149-483b-8e26-b0adfae0ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program Technical indicators for better trading decisions\n",
    "# feature engineering\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['Returns'] = data['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7785f776-8b8a-4138-a967-185d4820b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN values and reset index\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "80d250c6-fae0-49f9-92a6-d69ca324c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actions\n",
    "ACTIONS = {0: \"HOLD\", 1: \"BUY\", 2: \"SELL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a470f0c8-b983-47af-9d20-da6e1853c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the reinforcement training model, we extract with get state function\n",
    "# get state function\n",
    "def get_state(data, index):\n",
    "    return np.array([\n",
    "        data['Close'].iloc[index].item(),\n",
    "        data['SMA_5'].iloc[index].item(),\n",
    "        data['SMA_20'].iloc[index].item(),\n",
    "        data['Returns'].iloc[index].item()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5452e3fe-b4d3-4c1b-9fdb-f8e03a8b71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build trading environment\n",
    "# trading environment\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.initial_balance = 10000\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.index = 0\n",
    "        return get_state(self.data, self.index)\n",
    "\n",
    "    def step(self, action):\n",
    "        price = float(self.data.at[self.index, 'Close'])\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.balance >= price:  # BUY\n",
    "            self.holdings = self.balance // price\n",
    "            self.balance -= self.holdings * price\n",
    "        elif action == 2 and self.holdings > 0:  # SELL\n",
    "            self.balance += self.holdings * price\n",
    "            self.holdings = 0\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.data) - 1\n",
    "\n",
    "        if done:\n",
    "            reward = self.balance - self.initial_balance\n",
    "\n",
    "        next_state = get_state(self.data, self.index) if not done else None\n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "191dac75-bb78-4a66-96c9-7edda3346150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN is a neural network that approximates Q values for each state-action pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a4cb1bb1-2dd0-4184-996b-31963267dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network with torch library\n",
    "# deep q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "35511a00-7550-42f8-a917-ca5b7c78c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program a DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(list(ACTIONS.keys()))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                target += self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
    "\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            target_tensor = self.model(state_tensor).clone().detach()\n",
    "            target_tensor[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state_tensor)\n",
    "            loss = self.criterion(output, target_tensor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "5003b4a8-ca73-4517-a92b-fc3316775564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for its capabilities\n",
    "# Program the environment as well\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.index = 0\n",
    "        return get_state(self.data, self.index)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: 0 = HOLD, 1 = BUY, 2 = SELL\n",
    "        \"\"\"\n",
    "        price = self.data['Close'].iloc[self.index].item()\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.balance >= price:  # BUY\n",
    "            self.holdings += 1\n",
    "            self.balance -= price\n",
    "\n",
    "        elif action == 2 and self.holdings > 0:  # SELL\n",
    "            self.holdings -= 1\n",
    "            self.balance += price\n",
    "            reward = price  # reward for selling\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.data) - 1\n",
    "\n",
    "        next_state = get_state(self.data, self.index) if not done else None\n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6cdf2cbb-7913-4c3f-bec9-b8382238f3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Balance after testing: $10568.47\n",
      "Total Profit: $568.47\n"
     ]
    }
   ],
   "source": [
    "# create a new environment situation for testing\n",
    "test_env = TradingEnvironment(data)\n",
    "state = test_env.reset()\n",
    "done = False\n",
    "\n",
    "# simulate a trading session using the trained agent\n",
    "while not done:\n",
    "    # always choose the best action (exploitation)\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = test_env.step(action)\n",
    "    state = next_state if next_state is not None else state\n",
    "# Calculation with the reward function(s)\n",
    "final_balance = test_env.balance\n",
    "profit = final_balance - test_env.initial_balance\n",
    "print(f\"Final Balance after testing: ${final_balance:.2f}\")\n",
    "print(f\"Total Profit: ${profit:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
